{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import simfin as sf\n",
    "from simfin.names import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import r2_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pulling (from SimFin) --> api_calls.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'MbOGeJgi6qQjgYbb58oBVQDaObxEZzXg'\n",
    "\n",
    "# SimFin data-directory.\n",
    "sf.set_data_dir('~/simfin_data/')\n",
    "# SimFin load API key or use free data.\n",
    "sf.load_api_key('MbOGeJgi6qQjgYbb58oBVQDaObxEZzXg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = 'us'\n",
    "\n",
    "# Add this date-offset to the fundamental data such as\n",
    "# Income Statements etc., because the REPORT_DATE is not\n",
    "# when it was actually made available to the public,\n",
    "# which can be 1, 2 or even 3 months after the Report Date.\n",
    "offset = pd.DateOffset(days=60)\n",
    "\n",
    "# Refresh the fundamental datasets (Income Statements etc.)\n",
    "# every 30 days.\n",
    "refresh_days = 30\n",
    "\n",
    "# Refresh the dataset with shareprices every 10 days.\n",
    "refresh_days_shareprices = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = sf.StockHub(market=market, offset=offset,\n",
    "                  refresh_days=refresh_days,\n",
    "                  refresh_days_shareprices=refresh_days_shareprices)\n",
    "\n",
    "df_fin_signals = hub.fin_signals(variant='daily')\n",
    "df_growth_signals = hub.growth_signals(variant='daily')\n",
    "df_val_signals = hub.val_signals(variant='daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning/Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the DataFrames.\n",
    "dfs = [df_fin_signals, df_growth_signals, df_val_signals]\n",
    "df_signals = pd.concat(dfs, axis=1)\n",
    "\n",
    "# Remove all rows with only NaN values.\n",
    "df = df_signals.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "# List of the columns before removing any.\n",
    "columns_before = df_signals.columns\n",
    "\n",
    "# Threshold for the number of rows that must be NaN for each column.\n",
    "thresh = 0.75 * len(df_signals.dropna(how='all'))\n",
    "\n",
    "# Remove all columns which don't have sufficient data.\n",
    "df_signals = df_signals.dropna(axis='columns', thresh=thresh)\n",
    "\n",
    "# List of the columns after the removal.\n",
    "columns_after = df_signals.columns\n",
    "\n",
    "# Show the columns that were removed.\n",
    "columns_before.difference(columns_after)\n",
    "\n",
    "# Name of the new column for the returns.\n",
    "TOTAL_RETURN_1_3Y = 'Total Return 1-3 Years'\n",
    "\n",
    "# Calculate the mean log-returns for all 1-3 year periods.\n",
    "df_returns_1_3y = \\\n",
    "    hub.mean_log_returns(name=TOTAL_RETURN_1_3Y,\n",
    "                         future=True, annualized=True,\n",
    "                         min_years=1, max_years=3)\n",
    "\n",
    "dfs = [df_signals, df_returns_1_3y]\n",
    "df_sig_rets = pd.concat(dfs, axis=1)\n",
    "\n",
    "# Clip the signals and returns at their 5% and 95% quantiles.\n",
    "# We do not set them to NaN because it would remove too much data.\n",
    "df_sig_rets = sf.winsorize(df_sig_rets)\n",
    "\n",
    "# Remove all rows with missing values (NaN)\n",
    "# because scikit-learn cannot handle that.\n",
    "df_sig_rets = df_sig_rets.dropna(how='any')\n",
    "\n",
    "# Remove all tickers which have less than 200 data-rows.\n",
    "df_sig_rets = df_sig_rets.groupby(TICKER) \\\n",
    "                .filter(lambda df: len(df)>200)\n",
    "\n",
    "# List of all unique stock-tickers in the dataset.\n",
    "tickers = df_sig_rets.reset_index()[TICKER].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the tickers into training- and test-sets.\n",
    "tickers_train, tickers_test = \\\n",
    "    train_test_split(tickers, train_size=0.8, random_state=1234)\n",
    "\n",
    "df_train = df_sig_rets.loc[tickers_train]\n",
    "df_test = df_sig_rets.loc[tickers_test]\n",
    "\n",
    "# DataFrames with signals for training- and test-sets.\n",
    "X_train = df_train.drop(columns=[TOTAL_RETURN_1_3Y])\n",
    "X_test = df_test.drop(columns=[TOTAL_RETURN_1_3Y])\n",
    "\n",
    "# DataFrames with stock-returns for training- and test-sets.\n",
    "y_train = df_train[TOTAL_RETURN_1_3Y]\n",
    "y_test = df_test[TOTAL_RETURN_1_3Y]\n",
    "\n",
    "# List of signal names.\n",
    "signal_names = X_train.columns.values\n",
    "\n",
    "# List of signal names where spaces are replaced with _\n",
    "signal_names_ = [s.replace(' ', '_') for s in signal_names]\n",
    "\n",
    "# Column-name.\n",
    "FEATURE_IMPORTANCE = 'Feature Importance'\n",
    "\n",
    "def compare_feature_imp_corr(estimator):\n",
    "    \"\"\"\n",
    "    Return a DataFrame which compares the signals' Feature\n",
    "    Importance in the Machine Learning model, to the absolute\n",
    "    correlation of the signals and stock-returns.\n",
    "\n",
    "    :param estimator: Sklearn ensemble estimator.\n",
    "    :return: Pandas DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Wrap the list of Feature Importance in a Pandas Series.\n",
    "    df_feat_imp = pd.Series(estimator.feature_importances_,\n",
    "                            index=signal_names,\n",
    "                            name=FEATURE_IMPORTANCE)\n",
    "\n",
    "    # Concatenate the DataFrames with Feature Importance\n",
    "    # and Return Correlation.\n",
    "    dfs = [df_feat_imp, df_corr_returns]\n",
    "    df_compare = pd.concat(dfs, axis=1, sort=True)\n",
    "\n",
    "    # Sort by Feature Importance.\n",
    "    df_compare.sort_values(by=FEATURE_IMPORTANCE,\n",
    "                           ascending=False, inplace=True)\n",
    "\n",
    "    return df_compare\n",
    "\n",
    "def print_tree(estimator, max_depth=6, **kwargs):\n",
    "    \"\"\"\n",
    "    Print the first Decision Tree from a Random Forest.\n",
    "    :param estimator: Sklearn ensemble estimator.\n",
    "    \"\"\"\n",
    "    s = export_text(estimator.estimators_[0],\n",
    "                    max_depth=max_depth,\n",
    "                    feature_names=signal_names_,\n",
    "                    **kwargs)\n",
    "    print(s)\n",
    "\n",
    "# Parameters for scikit-learn's Random Forest models.\n",
    "model_args = \\\n",
    "{\n",
    "    # Random Forest parameters to adjust between\n",
    "    # over- and under-fitting.\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 15,\n",
    "    'min_samples_split': 100,\n",
    "    'min_samples_leaf': 10,\n",
    "\n",
    "    # Use all available CPU cores.\n",
    "    'n_jobs': -1,\n",
    "\n",
    "    # Set random seed to make the experiments repeatable.\n",
    "    'random_state': 1234,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ğŸ‹ğŸ¿ğŸ‹ğŸ¿ğŸ‹ğŸ¿ğŸ‹ğŸ¿ğŸ‹ğŸ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the estimator, but don't do any computations yet.\n",
    "regr = RandomForestRegressor(**model_args)\n",
    "\n",
    "# Fit the estimator to the training-data.\n",
    "# This may take several minutes on a 4-core CPU.\n",
    "_ = regr.fit(X=X_train, y=y_train)\n",
    "\n",
    "print_tree(regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing ğŸ’¯ğŸ’¯ğŸ’¯ğŸ’¯ğŸ’¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
